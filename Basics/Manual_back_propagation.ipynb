{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation is one of most fundamental element of deep learning. So having cristal clear understanding is mandatory here. Today we are going to implement back prop from scratch. To understand to the underhood of back propagation lets consider a very simple neural network. where we have only 2 input, 1 hidden layer consisting of 2 neuron and 2 output.![neural_network-7.png](http://puzzlemusa.com/wp-content/uploads/2018/04/neural_network-2-vs-2.png)\n",
    "[image source](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our NN for 2 given input we will expect some output. Here we will initialize the weights and biases of our neural net randomly. So our goal will be optimize those weights so that we can get less error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural net consist of few basic steps. Forward pass, calculate the error, backward pass, adjust weights and repeat. \n",
    "Lets initialize the imput, output, weights and biases.\n",
    "First we need to import out only dependency: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets call out inputs i1 and i2, \n",
    "hidden layer weights w1,w2,w3,w4, \n",
    "output layer weights w5, w6, w7, w8, \n",
    "biases b1 and b2\n",
    "outputs o1 and o2.\n",
    "For input matrix x we are expecting matrix y. We are gonna initialize them. We can initialize our weights and biases randomly. But simplycily and same error lets declare them hardcoded for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[.40],\n",
    "              [.60]])\n",
    "y = np.array([[.10],\n",
    "              [.90]])\n",
    "\n",
    "weights0 = np.array([[.10, .20],\n",
    "                     [.30, .40]])\n",
    "weights1 = np.array([[.50, .60],\n",
    "                     [.70, .80]])\n",
    "b1 = .25\n",
    "b2 = .75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our activation function we gonna use Sigmoid. Which is $$ x = \\frac{1}{(1 + e^{-1})} $$\n",
    "And the derivative of Sigmoid is $$ x` = x * (1 - x) $$\n",
    "Lets declare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, deriv=False):\n",
    "    if (deriv):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "In forward pass in each layer we will calculate the result, go through an activation function and pass the output to next layer. Lets see how we gonna calculate result.\n",
    "What will be the out put of h1?<br>\n",
    "We all know the equation of calculating output of a neuron, which is $$ y = wx + b $$\n",
    "Here y is the output, w is the weight, x is the input and b is bias.\n",
    "So $$ h1_{total} = (w1 * i1) + (w2 * i2) + b1 $$\n",
    "Same as\n",
    "$$ h2_{total} = (w3 * i1) + (w4 * i2) + b1 $$\n",
    "Now we need to pass through our activation function. What will be the output from h1?<br>\n",
    "$$ h1_{out} = \\frac{1}{(1 + e^{-h1_{total}})} $$\n",
    "Same as \n",
    "$$ h2_{out} = \\frac{1}{(1 + e^{-h2_{total}})} $$\n",
    "Now we out from out hidden layer. Lets pass them to our output layer and calcute.\n",
    "As we've seen earlier:\n",
    "Lets implement them.\n",
    "$$ o1_{total} = (w5 * h1_{out}) + (w6 * h2_{out}) + b2 $$\n",
    "Same as\n",
    "$$ o2_{total} = (w7 * h1_{out}) + (w8 * h2_{out}) + b2 $$\n",
    "Then\n",
    "$$ o1_{out} = \\frac{1}{(1 + e^{-o1_{total}})} $$\n",
    "$$ o2_{out} = \\frac{1}{(1 + e^{-o2_{total}})} $$\n",
    "\n",
    "As we've got our final output lets calculte the error by [MSE(mean squared error)](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "$$ MSE = \\frac{1}{n}\\int_{i=1}^{n}(Y_i - Y_i`)^2$$\n",
    "\n",
    "Enough jargon, lets implements this in just few lines code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(l2):\n",
    "    return np.square(np.subtract(y, l2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward():\n",
    "    l0 = x # input layer\n",
    "    l1 = sigmoid(np.dot(weights0, l0) + b1) # hidden layer\n",
    "    l2 = sigmoid(np.dot(weights1, l1) + b2) # output layer\n",
    "    return l0, l1, l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go to backward direction to update our weights. So lets start from w5. How mucu will error change if we change w5 a bit? Thats mean what is $$ \\frac{∂E}{∂w5} $$\n",
    "From the [chain rule](https://en.wikipedia.org/wiki/Chain_rule):\n",
    "$$ \\frac{∂E}{∂w5} = \\frac{∂E}{∂o1_{out}} * \\frac{∂o1_{out}}{∂o1_{total}} * \\frac{∂o1_{total}}{∂w5} $$\n",
    "First we need to find $$\\frac{∂E}{∂o1_{out}}$$\n",
    "$$ E = \\frac{1}{2}((y_1 - o1_{out})^2 + (y_2 - o2_{out})^2) $$\n",
    "$$ \\frac{∂E}{∂o1_{out}} = o1_{out} - y_1 \\text {or we can say it as our error delta} $$\n",
    "Now what is $$\\frac{∂o1_{out}}{∂o1_{total}}$$\n",
    "Well this is just the derivative of our activation function. So:\n",
    "$$ \\frac{∂o1_{out}}{∂o1_{total}} = o1_{out}*(1 - o1_{out}) $$\n",
    "Then what is value of $$\\frac{∂o1_{total}}{∂w5}$$\n",
    "We know $$ o1_{total} = (w5 * h1_{out}) + (w6 * h2_{out}) + b2 $$\n",
    "$$\\frac{∂o1_{total}}{∂w5} = h1_{out}$$\n",
    "Finally\n",
    "$$ \\frac{∂E}{∂w5} = (o1_{out} - y_1) * (o1_{total}*(1 - o1_{total})) * h1_{out} $$\n",
    "Following this equation we can solve the deltas of the weights of our output layer.\n",
    "Now we need find deltas for the weights of our hidden layer.\n",
    "Lets say h1. So what will be $$\\frac{∂E}{∂w1}$$\n",
    "$$\\frac{∂E}{∂w1} = \\frac{∂E}{∂h1_{out}} * \\frac{∂h1_{out}}{∂h1_{total}} * \\frac{∂h1_{total}}{∂w1} $$\n",
    "Lets find out these 3 derivatives.\n",
    "One very important thing to notice here from neuron h1 we have 2 connection to 2 output as well as 2 error(though we have already calculated our error all togather)\n",
    "That means:\n",
    "$$ \\frac{∂E}{∂h1_{out}} = ((\\frac{∂E}{∂w5} * w5) + (\\frac{∂E}{∂w7} * w7) $$\n",
    "Then $$ \\frac{∂h1_{out}}{∂h1_{total}} = h1_{out} * (1 - h1_{out})$$\n",
    "and $$ \\frac{∂h1_{total}}{∂w1} = i1$$\n",
    "Lets implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(l0, l1, l2):\n",
    "    error_delta = l2 - y\n",
    "    l2_delta = l1 * error_delta * sigmoid(l2, True)\n",
    "    l1_error = l2_delta * weights1\n",
    "    l1_delta = l0 * l1_error * sigmoid(l1, True)\n",
    "    return l1_delta, l2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got out deltas, now lets update the weight.\n",
    "Wait!\n",
    "Dont forget about the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .5\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.2524400928185405\n",
      "Error: 0.007442698634002469\n",
      "Error: 0.0012832914889168532\n",
      "Error: 0.00038502159427395506\n",
      "Error: 0.0001405640746316794\n",
      "Error: 5.6427736960854585e-05\n",
      "Error: 2.3882950241402928e-05\n",
      "Error: 1.0435222431570452e-05\n",
      "Error: 4.6516550288087706e-06\n",
      "Error: 2.1005652306555507e-06\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    l0, l1, l2 = forward()\n",
    "    if(i % 100 == 0):\n",
    "        print('Error: ' + str(MSE(l2)))\n",
    "    l1_delta, l2_delta = backward(l0, l1, l2)\n",
    "    weights1 -= (l2_delta * lr)\n",
    "    weights0 -= (l1_delta * lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
