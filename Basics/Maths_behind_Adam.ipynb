{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the maths we will solve y = ax + b. If you consider a = 5 and b = 10 then for some value of x we will get some y. Lets define our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(a, b, x):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some other functions to calculate MSE(Mean squared errors). Here y_hat is for some given input x and output y if we presume a random value for a and b then y_hat is the ourput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_hat, y):\n",
    "    return ((y - y_hat) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(a, b, x, y):\n",
    "    return mse(lin(a, b, x), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned we need some value of x and y. Lets define a function to get some fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_fake_data(n, a, b):\n",
    "    x = np.random.uniform(0, 1, n)\n",
    "    y = lin(a, b, x) + 0.1 * np.random.normal(0, 3, n)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our function for calculating error and getting data. Lets get some fake data and see the error if we choose some different a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.74562285385427"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "x, y = gen_fake_data(bs, 2., 10.)\n",
    "a_guess, b_guess = 1., 1.\n",
    "mse_loss(a_guess, b_guess, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here comes the interesting part.\n",
    "As we saw earlier $$ error = (y-y`)^2  $$\n",
    "                         $$= (y`-(ax+b))^2 \\text{  as  } y`=ax+b$$\n",
    "                         $$= y^2+(ax+b)^2-2y(ax+b)$$\n",
    "                         $$= y^2+a^2x^2+2axb+b^2-2axy-2by$$\n",
    "\n",
    "Now if we change the value of a bit what will be its impact on error?\n",
    "$$ \\frac{de}{da}=\\frac{d}{da} (y^2+a^2x^2+2axb+b^2-2axy-2by) $$\n",
    "               $$ =2x^2a+2xb-2xy $$\n",
    "               $$ =2x(ax+b-y) $$\n",
    "               $$ =2x(y`-y) $$\n",
    "And if we change the value of b bit what will be its impact on error?\n",
    "$$ \\frac{de}{db}=\\frac{d}{db} (y^2+a^2x^2+2axb+b^2-2axy-2by) $$\n",
    "               $$ =2ax+2b-2y $$\n",
    "               $$ =2(ax+b-y) $$\n",
    "               $$ =2(y`-y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are implementing function to calculate our derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivatives():\n",
    "    global a_guess, b_guess\n",
    "    y_pred = lin(a_guess, b_guess, x[index])\n",
    "    dedb = 2 * (y_pred - y[index])\n",
    "    deda = dedb * x[index]\n",
    "    return deda, dedb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to define a learning rate. Then we will able get our new value of a and b from our derivatives and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd():\n",
    "    global a_guess, b_guess\n",
    "    deda, dedb = get_derivatives()\n",
    "    a_guess -= (deda * lr)\n",
    "    b_guess -= (dedb * lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD is not that efficient when the surface curve is more steep. So we use a hyperameter called momentum. Basically momentum is we should keep going the way we are going and update a bit. We calculate it by the [linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) of the derivatives of our last value(a and b) and the direction we were going last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .9\n",
    "alpha = 1 - beta\n",
    "mga = 1.\n",
    "mgb = 1.\n",
    "\n",
    "\n",
    "def momentum():\n",
    "    global a_guess, b_guess, mga, mgb\n",
    "    deda, dedb = get_derivatives()\n",
    "    a_guess -= (mga * lr)\n",
    "    b_guess -= (mgb * lr)\n",
    "    mga = (beta * mga) + (alpha * deda)\n",
    "    mgb = (beta * mgb) + (alpha * dedb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we updating out values of a and b certain time we will understand that we are updating our values very slowly. To fix that father of neural network Geoffrey Hinton suggest that we should  divide the learning rate by an exponentially decaying average of squared gradients which is called RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "srda = 1.\n",
    "srdb = 1.\n",
    "\n",
    "\n",
    "def rmsprop():\n",
    "    global a_guess, b_guess, srda, srdb\n",
    "    deda, dedb = get_derivatives()\n",
    "    a_guess -= (deda * lr / np.sqrt(srda))\n",
    "    b_guess -= (dedb * lr / np.sqrt(srdb))\n",
    "    srda = ((beta * srda) + (alpha * deda)) ** 2\n",
    "    srdb = ((beta * srdb) + (alpha * dedb)) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally Adam(Adaptive Moment Estimation) is the combination of RMSProp and Momentum. Basically adam prevents the roughness of RMSProp. How? In adam we use 2 momentum. One is the momentum of our gradient and another is the momentum of our squared gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada1 = 1.\n",
    "adb1 = 1.\n",
    "ada2 = 1.\n",
    "adb2 = 1.\n",
    "beta1 = .7\n",
    "alpha1 = 1 - .7\n",
    "\n",
    "def adam():\n",
    "    global a_guess, b_guess, ada1, adb1, ada2, adb2\n",
    "    deda, dedb = get_derivatives()\n",
    "    a_guess -= (ada1 * lr / np.sqrt(ada2))\n",
    "    b_guess -= (adb1 * lr / np.sqrt(adb2))\n",
    "    ada1 = (beta1 * ada1) + (alpha1 * deda)\n",
    "    adb1 = (beta1 * adb1) + (alpha1 * dedb)\n",
    "    ada2 = ((beta * ada2) + (alpha * deda)) ** 2\n",
    "    adb2 = ((beta * adb2) + (alpha * dedb)) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our each optimization technique performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 4.91647177550759 b: 1.0 error: 54.66376869315951\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "lr = .01\n",
    "bs = 10\n",
    "epoch = 3\n",
    "a_guess, b_guess = 1., 1.\n",
    "for i in range(epoch):\n",
    "    for j in range (bs-1):\n",
    "        sgd()\n",
    "        #momentum()\n",
    "        #rmsprop()\n",
    "        #adam()\n",
    "        \n",
    "print(\"a: \" + str(a_guess) + \" b: \" + str(b_guess) + \" error: \" + str(mse_loss(a_guess, b_guess, x, y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
